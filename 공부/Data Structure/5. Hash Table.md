# 1. MIT OpenCourseWare
In Python
Dictionary : Abstract Data Type(ADT)
maintain set of items, each with a key
insert(item) : (overwrite any existing key)
delete(item)
search(key) : return item with given key or report doesn't exist

Searching item in Dictionary is O(log n) via AVL

Motivation
- docdist
- database
- compilers & interpreters
- network router/server
- substring search
- string commonalities
- file/directory synchronization
- cryptography

Simple approach
Direct-access Table
- store item in array indexed by key
- Badness :
	- keys may not be nonneg. integers(음이 아닌 정수)
	- gigantic memory hog
- Solution :
	- prehash 
		- maps keys to nonneg. integers
		- in Theory, keys are finite & discrete (String of bits)
		- in Python, hash(x) is the prehash of x
		- hash('\0B') = hash('\0\0C') = 64
		- ideally : hash(x) = hash(y) <=> x = y
	- hashing
		- reduce universe U of all keys(integers) down to reasonable size m for table
		- idea - m = O(n) (keys in dict)
		- h(x), h(y) => direct same thing => collusion!!  => Solution  == Chaining

>Chaining
> when h(x), h(y) direct same address, use Linked List.
> that address point Linked List. when caused collusion, add, delete, search Node.
> In worst case, searching item is O(n).
> summary - use Linked List in hash table.

Simple uniform hashing
each key is equally likely to be hashed to any slot of the table, independent of where other keys hashing
Analysis :
	expected length of chain for n keys, m slots = n/m = a = load factor
	=> running time = O(1 + chainging) = O(1 + a)


Hash functions
1. division method:
	1. h(k) = k mod m
2. multiplication method:
	1. h(k) = a*k mod 2^w >>(w-r).  k= w bit
	2. m = 2^r
	3. ![[Pasted image 20241009211246.png]]
3. Universal hashing:
	1. h(k) = {(ak + b) mod p mod} mod m
	2. a, b = random / p = prime number
	3. worst case keys k1 != k2
		P{h(k1) = h(k2)} = 1/m
		결국 슬롯의 개수로 index가 결정되기 때문에 같을 확률은 1/m이 된다. 따라서 chaining으로 인해 계속 길어질 수 있다.


## How to choose m?
want
- m = O(n)
- $\alpha$ = O(1)
idea :
	start small - m = 8
	grow/shrink as necessary
	----------
	# Table Doubling
	if n > m grow table
	m' = m+1 : cost of n inserts = $\Theta$(1+2+3...+n) = $\Theta$(n^2)
	m' = 2m : cost of n inserts = $\Theta$(1+2+4+8+...n) = $\Theta$(n)
	메모리를 할당하고 다시 해쉬를 한다.
	grow table : m -> m'
	- make table of size m'
	- build new hash h'
	- rehash for item in T
		T'.insert(item)
	- $\Theta$(n+m+m')
	# Amortization
	operation takes "T(n) amortized"
	if K operations take <= k$\times$T(n) time
	think of meaning ~"T(n) on average, where average over all operations
	table doubling : k inserts take $\Theta$(k) time => $\Theta$(1) amortized/insert
	(amortize를 한달 렌트하는 비용을 매일 계산하는 것으로 비유함.)

>Amortization
>영어강의가 이해가 잘 되지 않아 일부 다시 찾아봄https://www.cs.cornell.edu/courses/cs312/2008sp/lectures/lec20.html
>Resizable hash tables and amortized analysis 부분을 보면
>최악의 경우 size가 1인 array에 n = 2^k 만큼의 수를 삽입한다고 해보자.
>마지막 삽입시 n개의 요소가 rehash되고 그 전에는 n/2개의 요소, 그전전에는 n/4개의 요소, ... 가 rahash될것 이다. 이것을 구해보면 _n_ + _n_/2 + _n_/4 + _n_/8 + ... = _n_(1 + 1/2 + 1/4 + 1/8 + ...) = 2n가 되고 직접 추가한 n번의 삽입에서 해쉬가 된것을 더하면 3n의 해쉬가 이루어진다.
>따라서, 1개의 요소를 추가할때 요소당 최대 3번의 해쉬가 이루어지게 된다. 요소를 삽입하는데 필요한 시간은 평균적으로 O(1)이 되며, O(1) amortized run time을 가진다고 한다.


Deletion
- if m = n/2 then shrink -> m/2
	- slow : 2^k insert <-> 2^k + 1 insert
	- => $\Theta$(n) per operation
- if m = n/4 the shrink => m/2
	- amortiazed time -> $\Theta$(1)


search string with hash table (karp-rabin argorithm)
주어진 전체 String을 하나씩 움직이면서 주어진 패턴과 hash이 일치하는지 확인하고 hash가 일치할 경우 실제값이 일치하는지 확인하여 같은 String을 찾는 알고리즘



# Open addressing - Other professor

- no chaining
- one item per slot m (slot) >= n (element)

## probing
Hash function specified order of slots to probe for a key (for insert/search/delete)
h: U (universe of keys) $\times$ {0, 1, ..., m-1} (trial count) -> {0, 1, ..., m-1}
	h(k,1) h(k,2) ... h(k, m-1)
	arbitrary key k

ex)
hash table

| 0   |                                        |
| --- | -------------------------------------- |
| 1   | 586                                    |
| 2   | 133                                    |
| 3   | <span style="color : red"> 496 </span> |
| 4   | 204                                    |
| 5   |                                        |
| 6   | 481                                    |
| 7   |                                        |

현 상황에서 Insert(496)을 하려고 한다. h(496,1) = 4으로 hash function의 결과로 6번 슬롯이 결정됐으나, 이미 점유되어 있다. h(496,2) = 1로 역시나 점유되어 있다. h(496,3) = 3으로 3번 슬롯에 496을 삽입하게 된다.


- None : empty slot (flag)
- Insert(k, v)
	- keep probing until an empty slot is found. Insert item when found.
- Search(k)
	- As long as the slots encountered are occupied by keys $\neq$ k, keep probing until you either encounter k or find the empty slot.
- Delete(k)
	- Replace deleted item with DeleteMe flag(different from None)
	- Insert treat to DeleteMe(flag) the same as None. But, Search keeps going (treats diff form None)


## Probing Strategies

Linear probing h(k,i) = (h'(k) + i) mod m 
	 $\uparrow$ ordinary hash function, permutation
Cluster : consecutive grows of occupied slots which keep longer
	연속적으로 슬롯을 찾는다. 이렇게 되면 계속 hash를 이용하게 되더라도 constant time이 아니라 선형 또는 그 이상의 시간을 필요로 하게 된다.
cluster h(k, i)
0.01 < $\alpha$ = $\frac{n}{m}$ < 0.99.   $\Theta$(log n) size


Double hashing
h(k,i) = (h_1(k) + i * h_2(k)) mod m
if h_2(k) is relatively prime to m -> permutation
m = 2^r, h_2(k) for all k is odd.

Uniform Hashing Assumption
Each key is equelly likely to have any one of the m! permutations as its probe sequence.
$\alpha$ = $\frac{n}{m}$ Cost of operations insert <= $\frac{1}{1-\alpha}$
	알파가 너무 크다면 (ex. 0.99) 평균 100번의 probe가 필요해 진다. 상수지만 매우 나쁜 상수 아닌가?
	Open addressing은 구현도 쉽고 메모리도 적게 차지하지만 알파가 0.5가 넘지 않는 정도일때 사용하는 것을 권한다.

Password Storage 에 사용된다.



# Pycon 2010

## How can Python lists access every one of their items with equal speed?


- RAM is a vast array
- Addressed by sequentail integers
- Its first address is zero!
- Easy to implement a list a top memory


### Dictionary in Python
The Three Rule
1. A dictionary is really a list
	1. ![[Pasted image 20241028202058.png]]
2. Keys are hashed to produce indexes
	1. Python lets you see hashing in action through the hash() builtin
	2. Quite similar values often have very different hashes
	3. Hashes look crazy, but the same value always returns the same hash!
	4. lookup: same 3 steps
		1. compute the hash
		2. truncate it
		3. look in that slot
3. If at first you don't succeed, try, try again.
	1. Collision : When two keys in a dictionary want the same slot
	2. ![[Pasted image 20241029143425.png]]
	3. Consequence
	4. Because Collisions move keys away from their natural hash values. key order is quite sensitive to dictionary history.  순서에 따라서 같지만 다른 dictionary가 된다.
	5. The lookup algorithm is actually more complicated than "hash, truncate, look"
	6. Not all lookups are created equal. Some finish at their first slot, Some loop over several slots.
	7. When deleting a key, you need to leave "dummy" keys.
	8. When a key is deleted, its slot cannot simply be marked as empty. Otherwise, any keys that collided with it would now be impossible to find! So we created a dummy key instead.
	9. Dicts refuse to get full
		1. To Keep collisions rare, dicts resize when only $\frac{2}{3}$full
		2. When < 50k entries, size x4
		3. when > 50k entries, size x2
		4. Life cycle as dictionary fills : Gradually more crowded as keys are added. Then suddenly less as dict resizes
	10. Average dictionary performance is excellent.
		1. 평균적으로 2~3회의 확인만으로도 원하는 데이터를 찾을 수 있다.
		2. 나쁜 dict는 검색횟수가 매우 많이 (20회 가까이 또는 그 이상) 증가하기도 한다.
	11. Because an insert can radically reorder a dictionary, key insertion is prohibited during iteration.
	12. Take-away
	13. Hopefully "the rules" now make a bit more sense and seem less arbitrary.
		1. Don't rely on order
		2. Don't insert while iterating
		3. Can't have mutable key
	14. Dictionaries trade space for time. If you need more space, there are alternatives.
	15. If your calss needs its own `__hash__()` method you now know how hashes should behave.
		1. Scatter bits like crazy
		2. Equal instanes **must** have equal hashes
		3. Must also implements `__eq__()`method
		4. Make hash and equality quick!

May your hashes be unique, Your hash tables never full, and may your keys rarely collide.



# PyCon 2017 - The Dictionary Even Mightier
파이썬을 현재는 잘 사용하지 않으므로 dict 즉, hash table의 개념에 대한 것은 이전 PyCon에서 학습함.
차후 파이썬을 주로 사용하게 된다면 보자.


# MIT - Randomization : Universal & Perfect Hashing

Dictionary - problem : Abstract Data Type(ADT)

Maintain set of items, each  with a key, subject to 
- insert( item )
- delete( item )
- search( key ) : find item with that key

Hashing :  O(1) expected time per operation, O(n) space
- u = # possible keys
- n = # keys currently in DS
- m = # slots in table
- h : {0,1,2,... u -1} -> {0,1,2,..., m-1}
Hashing with chaining achieves $\theta (1 + \alpha )$  ($\alpha = \frac{n}{m}$)
Assuming simple uniform hashing
	$P_r(k_1 \neq k_2)\{h(k_1) = h(k_2)\}=\frac{1}{m}$
Requires assuming keys are random (AVERAGE CASE)

hash, 1650s : cut into small pieces
- hacher(Fr.) chop up \[hatchet]
- hache(Old Fr.) axe



## Universal Hashing
- for dynamic Data
- choose h randomly from H 
- require H to be universal hash family : for all keys $k \neq k$
	- $p_r(h\in H)\{h(k) = h(k')\}<=\frac{1}{m}$

Theorem : for n arbitrary distinct keys
for random $h \in H$ (universal)
E\[#keys colliding in slot] $<= 1 + \alpha$

Proof : consider keys $k_1 , k_2 , ... , k_n$

let $I_{ij} = \begin{cases}1\;if\;h(k_i)=h(k_j)\\0\;else\end{cases}$

E\[#keys colliding with $k_i$] $\leq 1 + \alpha$
= E\[$\displaystyle\sum_{i\neq j}{I_{ij}} + I_{ii}$] 
=$\displaystyle\sum_{j\neq i}{E[I_{ij}]} + 1$
= $\displaystyle\sum_{j\neq i}{P_r\{I_{ij} = 1\} } + 1$
= $\displaystyle\sum_{j\neq i}{P_r\{h(k_1) = h(k_j)\}} + 1$
$\leq \displaystyle\sum_{j\neq i}{\frac{1}{m}} + 1$
$\leq \frac{n}{m} + 1$


### Bad universal hash family
H = {h:{0, 1,..., u-1} -> {0,1, ..., m-1}}
time & space = $\Theta(u)$

### Dot-product hash family
- assume m is prime
- assume $u = m^r$ for integer
- view key k in base m
	- k = {$k_0,k_1,...,k_{r-1}$} (0<$k_i$<m)
- for key a={$a_0,a_1,...,a_{r-1}$}
- define $h_a(k)$
- $= (a \cdot k)\; mod\; m$
- $=\sum_{i=0}^{r-1}{a_i k_i}\;mod\;m$
- H = $\{h_a | a \in \{0,1,...,u-1\}\}$
World RAM : manipulate O(1) words take O(1) time & data values (keys) fit in word
[[Dot Product Hash]]

Another:
$h_{a,b} = [(a \cdot k+b)\;mod \;p]\;mod \;m$
H = {$h_a | a \in \{0,1,...,u-1\}$}
$p > m$ prime



## Static Dictionary Problem
given n keys, support Search
Perfect Hashing :
- O(1) worst-case for search
- O(n) worst-case space
- Polynomial build time
Idea : 2-level hashing
Universal Hash is use LinkedList. List is so slow. One level is added instead of list. Set hash table with no colliding in that level.
1. pick $h_1 : \{0,1,...,m-1\}$ from universal hash family, $m = \Theta (n)$
2. for each slot $j \in \{0,1,...,m-1\}$ ;
	1. $l_j$ = \#keys (among H) hashing to slot 
	2. if $\sum_{j=0}^{m-1}{l_j^2} > c \cdot n$ then redo No.1
	3. pick $h_{2-j}\;:\{0,1,...,u-1\} \rightarrow \{0,1,...,l_j^{2}-1 \}$ from universal hash family
	4. while $h_{2,j}(k_i) = h_{2,j}(k_{i'})$ for some j $k_i \neq k_{i'}\;\; h_1(k_i)\neq h_1(k_{i'})$ repick $h_{2,j}$ => no collisions

> Birthday Paradox
> 사람이 많아 질수록 생일이 겹칠 확률이 급격히 올라간다는 이론.
> 이 이론에 따라 충돌확률을 1/2이하로 만들기위해서 제곱을 한다.

$P_r\{h_{2,j}(k_i) = h_{2,j}(k_{i'} for\; some \;i \neq i'\}$ $\leq$ $\sum_{i \neq i'}{P_r \{h_{2,j}(k_i) = h_{2,j}(k_{i'}\}}$ $\leq \frac{1}{l_j^2}$
=> 2 expected trails O(log n) trials with high probability.
$l_i$ = O(log n) with high probability for All i. 

No.3 & No.4 -> O(n $\log^2 {n}$)

No.2 : $E[\sum_{j=0}^{m-1}{l_j^2}]$ = $E[\sum_{i=1}^n \sum_{j=1}^n{I_{ij}}]$     $I_{ij} = \begin{cases}1\;if\;h_1(k_i)=h_1(k_{i'})\\0\;else\end{cases}$
 = $\sum_{i=1}^n \sum_{j=1}^n{E[I_{ij}]}$      $E[I_{ij}]$ = 1/m
 = n + $\frac{2(n, 2)}{m}$ = $\Theta (n)$ 
 
 


# MIT - Universal Hashing, Perfect Hashing
Dictionary : insert, delete, find
Model : "keys" are integers in {1,...,m} (some range) fit in one machine word + arithmetic.
Array - Too big
Hash function :  {1,...,m} -> {1,...s}
store in an array of size s
n items to store ( s >=  n)
problem : collisions h(x) = h(y) (linked list per bucket)
any specific h bad for some inputs


### Hash family
set of hashes $h_i$ s.t.(such that) for any inputs. some $h_i$ is good.

e.g. all possible functions map n -> s > n with no collisions?
sort the items, put i*th* smallest at position i.
problems : 
	construction : nlogn to shild table. (slow)
	representation : n logn bits $i_e$ n words (huge)
	look up : binary search (slow)

Randomize
	e.g. choose random function
	how good is it?
	if n -> n at random, max load(#items in bucket) = O($\frac{\log{n}}{\log{\log{n}}}$)
	on lookup, what si E\[#collisions]?
	indicator variable. $C_{ij} = \begin{cases}1\;if\;j\;lands \;in \;same\;bucket\;as\;i\\0\;otherwise\end{cases}$
	lookup time for i *th* item
	\#items in j's bucket = $\sum_{j}{C_{ij}}$
	E\[lookup] = E\[$\sum_{j}{C_{ij}}$] = $\sum_{j}{E[C_{ij}]}$  = $\frac{n-1}{s}$
	$E[C_{ij}]$ = $P_r{[C_{ij}=1]}$ = $P_r[j\;lands \;in \;i\;bucket]$ =$\frac{1}{s}$ 
	.
	Problem : space for fn : fn = $s^m \rightarrow\;m\log{b}$ bits
	Sol : use fewer function
		goal : describe in O(1) space -> $m^{O(1)}$ function
		Carter-Wegman. 2-universal hash family
		don't need full random
		just need pairwise item collisions to look random
		-> just need pairwise independence in placement
		$p_r[j\; goes\; to\; b^{'} \;| \;i\;go\; to\; b]$ = $P_r[j\;goes\;b^{'}]$ 
		step 1) pairwise independent map {1,2,...,m} -> {1,2,...,m}
		step 2) map random {1,2,...m} -> {1,2,...,s} by mod s
	Define $h_{ab}$	pick prime p     m< p < 2m
	$h_{ab}$ -> ($(ax+b)\;mod\;p$) mod s.  (step 1 = $(ax+b)\;mod\;p$)
	$m^2$ func
	pick random a,b
	Claim : if a,b rnadom then All keys x=y, $ax+b\;mod\;p$, $ay+b\;mod\;p$ are uniformly distributed pairwise independent
	Proof:
	$P_r[ax+b=u\;mod\;u\;and\;ay+b=t\;mod\;p]$
	$ax+1\cdot b =u$
	$ay+1\cdot b =t$
	$\begin{pmatrix} x &1\\ y&1\end{pmatrix} \begin{pmatrix}a \\ b\end{pmatrix} = \begin{pmatrix}u \\ t\end{pmatrix}\;mod\;p$ det=(x-y)$\neq0$ 
	exactly on pair a,b that produces u,t
	so $P_r[ax+b=u\;and\;ay+b=t]$ = $\frac{1}{p^2}$
	.
	mod s
	slight nonuniformity (1 + s/m)
	still pairwise independent
	Result : origin analysis still works
	constant space representation
	constant time evaluation
	expected O(1) lookups


### Perfect Hashing
0 collisions?
s >= n
s = O(n)
Fredman Komolos szemered
static case
e.g. random 1,2,...n -> 1,2,...,n has collisions.
more space? s>>n
random n -> s = $\theta(n^2)$
E\[total # collisions] = E\[$\sum_{i < j}{C_{ij}}$ \] = $\begin{pmatrix}n\\2\end{pmatrix} E[C_{ij}]$ $\cong \frac{n^2}{2s}$
e.g. s=$n^2$ gives E\[collision] = 1/2
markov mequality :  for random variable X >= 0, $P_r[X\geq t]\leq E[X]/t$
Application : $P_r[\# collisions \geq 1]$ $\leq \frac{1}{2}$
Amplifiction : Keep Trying
E\[#attempts]  = 2
$P_r[\geq k\;tries]$ $\leq 2^{-k}$ 
E\[work] =O(n) in expectation

FKS : 2 level hashing
step1 : hash n items to n buckets (2-independence)
step2 : perfect hash each bucket with b item to b^2 size hash table
-> 2 level perfect hash table
exactly 2 hash computation to find item
space use
top level : O(n)
level 2 : $\sum{b_i^2}$ i bucket size

E\[$\sum{b_k^2}$]
$b_k^2$=$\sum_{i,j \in b_k}{1}$ 
$\sum{b_k^2}$ = $\sum_{k}{\sum_{i,j \in b_k}{1}}$ 
= $\sum_{i,j}{[i,j\;in\;smae\;bucket]}$
=$\sum_{i,j}{C_{ij}}$
E\[] = E\[$\sum_{ij}{C_{ij}}$]=O($\frac{n^2}{s}$)=O(n) if s=n at top level


# Hash Table in 4
