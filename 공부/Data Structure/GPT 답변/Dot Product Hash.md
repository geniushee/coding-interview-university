### **Dot Product Hash의 정의**

Dot Product Hash는 다음과 같이 정의할 수 있습니다:

1. 주어진 입력 벡터 x와 랜덤하게 선택된 벡터 a의 내적을 계산.
2. 내적 결과를 특정 값으로 나누거나, 적절히 변환하여 해시 값으로 사용.

$h_{a,b}(x)=⌊(a⋅x+b)/w⌋$

여기서:

- x: 입력 벡터.
- a: 랜덤하게 선택된 벡터.
- b: 랜덤 오프셋(편향값).
- w: 슬라이딩 윈도우 크기(조절 가능한 값).
- ⌊⋅⌋: 내림(floor) 연산.

---

### **작동 원리**

1. **내적 계산**:
    
    - $a \cdot x = \sum_{i=1}^d a_i x_i$​ (두 벡터의 각 요소를 곱한 값들의 합).
    - 벡터 a는 해시 함수의 매개변수로서 랜덤하게 선택됩니다.
2. **분할 및 변환**:
    
    - 내적 결과에 b를 더해 오프셋을 적용한 후 w로 나눠 해시 값으로 변환.
    - w는 해시 테이블의 크기나 원하는 분포를 조절하는 역할을 합니다.
3. **충돌 감소**:
    
    - 서로 다른 입력 벡터 x와 y가 같은 해시 값을 가지도록 충돌할 확률을 최소화.

---

### **Dot Product Hash Family의 특징**

1. **유니버설 해싱 가능**:
    
    - 적절히 설계된 벡터 a, b, w를 사용하면 입력 데이터의 특정 패턴과 무관하게 충돌 확률을 균등하게 분산시킬 수 있음.
2. **고차원 데이터 처리**:
    
    - Dot Product는 고차원 벡터 공간에서 데이터의 특징을 추출하거나, 효율적인 해싱을 구현하는 데 유리.
3. **기하학적 응용**:
    
    - 벡터 간의 기하학적 관계(예: 각도, 거리)를 활용할 수 있음.

---

### **사용 사례**

1. **유사도 검색 (Similarity Search)**:
    
    - 고차원 데이터(예: 이미지, 텍스트 임베딩)의 유사도를 비교하기 위한 해싱.
    - 예: 로컬리티 민감 해싱(LSH, Locality-Sensitive Hashing)에서 사용.
2. **정보 검색 및 머신러닝**:
    
    - 대규모 데이터에서 특징 벡터를 빠르게 검색하거나 분류.
3. **암호학**:
    
    - 데이터를 고유하게 매핑하거나 무작위성을 부여.

---

### **장점**

- 고차원 데이터를 효과적으로 처리할 수 있음.
- 충돌 확률을 수학적으로 분석하고 제어할 수 있음.
- 유니버설 해싱의 요구 조건을 만족하기 쉬움.

---

### **단점**

- 랜덤 벡터 a와 파라미터 w의 선택에 따라 성능이 달라질 수 있음.
- 벡터 내적 계산 비용이 크면 성능 저하 가능.

---

### **예시**

만약 2차원 벡터 $x = (x_1, x_2)$와 $a = (a_1, a_2)$가 있고, b = 0, w = 10라면:

1. 내적 계산: $a \cdot x = a_1 x_1 + a_2 x_2$
2. 해시 값 계산: $h_{a,b}(x) = \lfloor (a_1 x_1 + a_2 x_2) / 10 \rfloor$.

이 방식은 고유한 벡터 조합을 해시 값으로 변환하며, 고차원 데이터에 대해 빠른 검색이 가능합니다.